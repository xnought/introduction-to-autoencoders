<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=1300" />
		<meta
			name="description"
			content="Learn the basic structure of autoencoders through training an autoencoder in the browser and with other interactive explanations."
		/>
		<!-- Share card -->
		<meta
			property="og:url"
			content="http://introduction-to-autoencoders.vercel.app/"
		/>
		<meta
			property="og:title"
			content="An Interactive Introduction to Autoencoders"
		/>
		<meta
			property="og:description"
			content="Learn the basic structure of autoencoders through training an autoencoder in the browser and with other interactive explanations."
		/>

		<title>Introduction to Autoencoders</title>

		<link rel="icon" type="image/ico" href="/favicon.ico" />
		<link rel="stylesheet" href="/styles.css" />
		<link rel="stylesheet" href="/build/bundle.css" />

		<script src="https://cdn.jsdelivr.net/npm/three@0.106.2/build/three.min.js"></script>
		<script src="/scatter-gl.min.js"></script>

		<script src="https://distill.pub/template.v2.js"></script>
		<link
			href="https://fonts.googleapis.com/icon?family=Material+Icons"
			rel="stylesheet"
		/>
		<script defer src="/build/bundle.js"></script>
	</head>

	<body>
		<d-front-matter>
			<script type="text/json">
				{
					"authors": [
						{
							"author": "Donald R Bertucci",
							"authorURL": "https://donnybertucci.com",
							"affiliation": "Oregon State University",
							"affiliationURL": "https://www.oregonstate.edu/"
						}
					]
				}
			</script>
		</d-front-matter>

		<!-- TITLE + DESCRIPTION -->
		<d-title>
			<h1>An Interactive Introduction to Autoencoders</h1>
			<p>Learn autoencoders by training one right in your browser!</p>
		</d-title>

		<!-- MAIN TOOL / VISUALIZATION -->
		<div id="main"></div>

		<!-- NAME AND AFFILIATION -->
		<d-byline></d-byline>

		<!-- THE ARTICLE PORTION USING DISTILL TEMPLATE -->
		<d-article>
			<!-- INTRODUCE THE TOPIC HERE -->
			<p id="intro">
				Autoencoders have many different applications, but most notably
				they have been used for dimensionality reduction and as
				generative models
				<d-cite key="baldi1989neural, baldi2012autoencoders"></d-cite>.
				Even though autoencoders are used to learn complex
				representations, the neural network architecture itself is
				<b> very simple</b>! Autoencoders just learn to
				<i>deconstruct</i> down, then learn to <i>reconstruct</i> back
				up.
			</p>
			<p>
				The rest of the article will dive into the
				<a href="#structure">Structure</a> of an autoencoder in context
				to <a class="figure-number-text" href="#main">1</a>. More
				specifically, it will be broken up into the
				<a style="color: var(--encoderFill)" href="#encoder">
					<img src="encoder.svg" width="10px" /> Encoder</a
				>,
				<a style="color: var(--latentFill)" href="#latent"
					>Latent Space</a
				>, and
				<a style="color: var(--decoderFill)" href="#decoder"
					>Decoder <img src="decoder.svg" width="10px"
				/></a>
				to explain each piece of the puzzle. Then, the article will end
				with a <a href="#conclusion">Conclusion</a> that extends to
				applications of autoencoders elsewhere.
			</p>

			<h2 id="structure">Structure</h2>

			<!-- ENCODER SECTION -->
			<h3 class="under-line encoder" id="encoder">Encoder</h3>
			<p>
				The <span class="encoderFill">Encoder</span> is the first half
				of the neural network that takes an input with a
				<b>higher</b> dimension, then outputs to a
				<b>lower</b>
				dimension â€“ thereby creating a bottleneck<d-footnote
					>More precisely, it takes the input of some dimension
					<d-math inline> m </d-math>

					, then creates a bottleneck by reducing the dimensions down
					to
					<d-math inline> n</d-math>, where
					<d-math inline> m > n</d-math>.</d-footnote
				>. Similar to other data compression problems, we are going from
				a larger to smaller representation<d-cite
					key="crocetti2017what"
				></d-cite
				>. Hence, the design of the
				<span class="encoderFill">Encoder</span> from
				<a class="figure-number-text" href="#main">1</a>
				<img src="encoder.svg" width="10px" /> trapezoid , starting with
				a larger base, moving to a smaller one: going from 3
				Dimensional(<code>3D</code>) to 2 Dimensional(<code>2D</code>)
				data.
			</p>
			<p>
				<d-figure id="encoderDiagram"> </d-figure>
				<figcaption style="grid-column: text">
					<a class="figure-number-text" href="#encoderDiagram">2</a>:
					Hover over data points in
					<span class="encoderStroke">3D input Data</span>
					or
					<span class="latentColor">2D Latent Space</span>
					to see encoder mapping from <code>3D</code> to
					<code>2D</code>.
				</figcaption>
			</p>
			<p>
				You can see from
				<a class="figure-number-text" href="#encoderDiagram">2</a> after
				training the entire autoencoder, the
				<span class="encoderFill">Encoder</span> is just a learned
				function that takes the input to a lower dimension. In the
				<a class="figure-number-text" href="#encoderDiagram">2</a>, the
				<span class="latentColor">2D Latent Space</span> looks like the
				<span class="encoderStroke">3D input Data</span> if we ignored
				the vertical dimension. This is exactly what we should expect
				given the bottleneck defined from <code>3D</code> to
				<code>2D</code>.
			</p>

			<!-- LATENT SPACE SECTION -->
			<h3 class="under-line latent" id="latent">Latent Space</h3>
			<p>
				The <span class="latentColor">Latent Space</span> is composed of
				all outputs from the <span class="encoderFill">Encoder</span>.
				Or in other words, one output is a latent vector, and all
				outputs would constitute a
				<span class="latentColor">Latent Space</span>. Among being able
				to do vector arithmetic to find connections and combinations,
				visualizing this space gives insight to the structure of the
				data.
			</p>
			<p>
				One way to visualize the structure that forms is through
				<span
					style="
						color: rgb(147, 1, 61);
						font-size: 15px;
						display: inline-block;
					"
					>O</span
				><span
					style="
						color: rgb(177, 34, 68);
						font-size: 15px;
						display: inline-block;
					"
					>p</span
				><span
					style="
						color: rgb(201, 64, 70);
						font-size: 15px;
						display: inline-block;
					"
					>p</span
				><span
					style="
						color: rgb(219, 92, 68);
						font-size: 15px;
						display: inline-block;
					"
					>o</span
				><span
					style="
						color: rgb(229, 124, 74);
						font-size: 15px;
						display: inline-block;
					"
					>s</span
				><span
					style="
						color: rgb(234, 156, 90);
						font-size: 15px;
						display: inline-block;
					"
					>i</span
				><span
					style="
						color: rgb(236, 186, 112);
						font-size: 15px;
						display: inline-block;
					"
					>t</span
				><span
					style="
						color: rgb(237, 210, 136);
						font-size: 15px;
						display: inline-block;
					"
					>e</span
				><span style="color: rgb(236, 226, 158); font-size: 15px">
				</span
				><span
					style="
						color: rgb(229, 233, 162);
						font-size: 15px;
						display: inline-block;
					"
					>G</span
				><span
					style="
						color: rgb(213, 228, 151);
						font-size: 15px;
						display: inline-block;
					"
					>r</span
				><span
					style="
						color: rgb(187, 217, 148);
						font-size: 15px;
						display: inline-block;
					"
					>a</span
				><span
					style="
						color: rgb(155, 204, 152);
						font-size: 15px;
						display: inline-block;
					"
					>d</span
				><span
					style="
						color: rgb(118, 188, 155);
						font-size: 15px;
						display: inline-block;
					"
					>i</span
				><span
					style="
						color: rgb(85, 166, 160);
						font-size: 15px;
						display: inline-block;
					"
					>e</span
				><span
					style="
						color: rgb(63, 136, 168);
						font-size: 15px;
						display: inline-block;
					"
					>n</span
				><span
					style="
						color: rgb(66, 105, 164);
						font-size: 15px;
						display: inline-block;
					"
					>t</span
				><span
					style="
						color: rgb(88, 74, 151);
						font-size: 15px;
						display: inline-block;
					"
					>s</span
				>

				<img src="trail.svg" alt="trail" />. By understanding what
				direction each point in the
				<span class="latentColor">Latent Space</span>
				is tending towards, we can get an idea of where the training is
				headed towards
				<d-cite key="kahng2018gan, kahng2018ganpaper"></d-cite>.
				<d-figure id="latentGradDiagram"> </d-figure>
			</p>
			<p>
				After computing the gradient of loss with respect to the latent
				output<d-footnote
					>The partial derivative of loss with respect to the first
					latent output
					<d-math inline>
						\frac{\partial \text{loss}}{\partial
						\text{latent}_0}</d-math
					>
					and the partial derivative of loss with respect to the
					second latent output
					<d-math inline>
						\frac{\partial \text{loss}}{\partial
						\text{latent}_1}</d-math
					>. </d-footnote
				>, we now have direction of steepest ascent to
				<b>increase</b> loss. Then, since the goal is to lower loss, we
				negate<d-footnote>
					The negation of the Gradient is where the term
					<b>Opposite</b> of "<b>Opposite</b> Gradient" refers
					to.</d-footnote
				>
				the direction to <b>decrease</b> loss. In
				<a class="figure-number-text" href="#latentGradDiagram">3</a>,
				the point has a trail that represents the Opposite Gradient:
				what direction the point needs to move to lower loss<d-cite
					key="kahng2018gan, kahng2018ganpaper"
				></d-cite
				>.
			</p>
			<p>
				<d-figure id="latentDiagram"> </d-figure>
				<figcaption style="grid-column: text">
					<a class="figure-number-text" href="#latentDiagram">4</a>:
					Drag the <span style="color: #ff6600">slider</span> to see
					<span class="latentColor">2D Latent Space</span> during
					training.
				</figcaption>
			</p>
			<p>
				Just by observing the trails
				<img src="trail.svg" alt="trail" />, you can see the structure
				that takes place over training. Notice how each point doesn't
				move exactly in the direction of it's trail
				<img src="trail.svg" alt="trail" />, it is more of an indicator
				of the gravity of the structure: larger and more trails
				<img src="trail.svg" alt="trail" /> will pull the data in that
				direction, and uniformly distributed trails
				<img src="trail.svg" alt="trail" />
				will not affect the structure at all. This method of
				visualization can be applied to other outputs, demonstrated by
				<code>Kahng <i>et al.</i></code> in
				<a href="https://poloclub.github.io/ganlab/">GAN Lab</a
				><d-cite key="kahng2018gan"></d-cite>. By specifically applying
				it to the <span class="latentColor">2D Latent Space</span> in
				<a class="figure-number-text" href="#latentDiagram">4</a>, it
				becomes much easier to see where the points want to move and
				understand the underlying structure.
			</p>

			<!-- DECODER SECTION -->
			<h3 class="under-line decoder" id="decoder">Decoder</h3>
			<p>
				After the <span class="encoderFill">Encoder</span>
				<b>deconstructs</b>
				the original input down to the
				<span class="latentColor">Latent Space</span>, the
				<span class="decoderFill">Decoder</span>
				<b>reconstructs</b> back up to the original input. Hence the
				<img src="decoder.svg" width="10px" /> trapezoid for the
				<span class="decoderFill">Decoder</span> design, starting with a
				smaller base, moving to a larger one. The loss function,
				adequately named "reconstruction loss," is computed with the
				original input and the reconstructed input. Now we can
				backpropagate from the reconstruction loss and optimize! All the
				pieces are now present to train the autoencoder.
			</p>
			<p>
				<d-figure id="decoderDiagram"> </d-figure>
				<figcaption style="grid-column: text">
					<a class="figure-number-text" href="#decoderDiagram">5</a>:
					Hover over data points in
					<span class="latentColor">2D Latent Space</span>
					or
					<span class="decoderStroke">3D Reconstruction</span>
					to see mapping from <code>2D</code> to <code>3D</code>.
				</figcaption>
			</p>
			<p>
				You can see from
				<a class="figure-number-text" href="#decoderDiagram">5</a> after
				training the entire autoencoder, the
				<span class="decoderFill">Decoder</span> is just a learned
				function that reconstructs from the bottleneck. In the
				<a class="figure-number-text" href="#decoderDiagram">5</a>, the
				<span class="decoderStroke">3D Reconstruction</span> looks like
				the <span class="latentColor">2D Latent Space</span> if we added
				a dimension.
			</p>

			<!-- AN EXAMPLE WITH DIGITS -->
			<h2 id="conclusion">Conclusion</h2>
			<p>
				Autoencoders are not just fixed to <code>3D</code> data like the
				previous examples. They can be used on other examples too!
			</p>
			<p>
				In fact, in addition to being applied to many different shapes
				and sizes of data, the autoencoder structure can be used to
				tackle real problems like denoising images, removing
				imperfections or watermarks from images, learning complex or
				emergent structures in data, and many more
				<d-cite
					key="xie2012image, hashisho2019underwater, cisneros2021visualizing"
				></d-cite
				>.
			</p>
			<p>
				To give one final example, if we wanted to visualize the
				structure of the
				<code>MNIST</code> digits dataset that consists of
				<code>28 by 28</code> handwritten digits (0-9), we could use an
				autoencoder!
			</p>
			<p>
				<d-figure id="mnist"> </d-figure>
				<figcaption>
					<a class="figure-number-text" href="#mnist">6</a>: Hover
					over <span class="latentColor">2D Latent Space</span> to see
					<span class="decoderStroke">Reconstruction</span> of a
					digit.
				</figcaption>
			</p>

			<p>
				In <a class="figure-number-text" href="#mnist">6</a>. after
				training the autoencoder with a <code>2D</code> bottleneck, we
				can see clusters form in the
				<span class="latentColor">Latent Space</span>! Also notice the
				similarity between the digits! See how the
				<span style="color: #16bece">9</span>s are seen mixed in with
				the <span style="color: #7f7f7f">7</span>s and
				<span style="color: #9467bd">4</span>s in the
				<span class="latentColor">Latent Space</span>.
			</p>

			<div class="hidden-citations">
				<d-cite
					key="karpathymnist, carter2017tensorflow, hohman2020communicating, kahng2018ganpaper, kahng2018gan, smilkov2019tensorflow, wattenberg2016use, baldi1989neural, baldi2012autoencoders, crocetti2017what"
				/>
			</div>
		</d-article>

		<d-appendix>
			<h3>Acknowledgments</h3>
			<p>
				The outcome was heavily influenced and inspired by the amazing
				works of
				<a href="https://poloclub.github.io/ganlab/">GAN Lab</a> and
				<a href="https://pair-code.github.io/understanding-umap/"
					>Understanding UMAP</a
				>
				<d-cite
					key="kahng2018gan, kahng2018ganpaper, coenen2019understanding"
				></d-cite
				>. Visualizing <code>3D</code> data reduced down to
				<code>2D</code>
				<d-cite key="coenen2019understanding"></d-cite>, the model view,
				the layered distributions, and the gradient lines/trails
				<d-cite key="kahng2018gan, kahng2018ganpaper"></d-cite>
				were key ideas used in the main autoencoder visualization.
			</p>

			<p>
				The article was styled with the
				<a href="https://github.com/distillpub/template"
					>Distill HTML Template</a
				>.
			</p>
			<p>
				<a href="https://pair-code.github.io/understanding-umap/"
					>Understanding UMAP</a
				>,
				<a
					href="https://distill.pub/2020/communicating-with-interactive-articles/"
					>Communicating with Interactive Articles</a
				>, and
				<a href="https://poloclub.github.io/ganlab/">GAN Lab</a>
				were used for a css styling reference for the article and
				controls.
			</p>
			<p>
				<b>Libraries used: </b>Plotting and visualization done in
				<a href="https://svelte.dev/">Svelte</a>, with help of
				<a href="https://d3js.org/">d3.js</a> and
				<a href="https://github.com/PAIR-code/scatter-gl">ScatterGL</a>.
				Autoencoder created and trained with
				<a href="https://www.tensorflow.org/js">Tensorflow JS</a>.
			</p>

			<h3>Author Contributions</h3>
			<p>
				<a href="https://www.donnybertucci.com/">Donald R Bertucci</a>
				is an undergraduate student at Oregon State university who
				implemented all of the visualizations and wrote the article.
			</p>

			<d-footnote-list></d-footnote-list>
			<d-citation-list></d-citation-list>
		</d-appendix>

		<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
		<d-bibliography src="bibliography.bib"></d-bibliography>
	</body>
</html>
